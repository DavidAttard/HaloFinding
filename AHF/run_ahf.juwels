#!/bin/bash

# The following bash script is a job submission to run AHF provided it has been compiled as Standard MPI+OpenMP
# The follwoing assumes that for each snapshot there is a file with a .input file inside wich can be generated using gen_ahf_input.py
# Note: You need to change the array values depending on the number of snaps to be analysed.

#SBATCH -A hestiaeor
#SBATCH --job-name="ahf"
#SBATCH --time=1:00:00
#SBATCH --nodes=20 #The number of nodes should be made greater or equal to the number of NcpuReading in AHF.inputfile 
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=90
#SBATCH --mem=0
# #SBATCH --partition=mem192
#SBATCH --partition=batch
#SBATCH --output=/p/scratch/hestiaeor/david/log/%x.%j.o
#SBATCH --error=/p/scratch/hestiaeor/david/log/%x.%j.e
#SBATCH --mail-type=ALL
#SBATCH --mail-user=da500@sussex.ac.uk
#SBATCH --array=0-2
#======START=====
#module load defaults/GPU
module load Stages/2024
module load GCC/12.3.0
module load ParaStationMPI/5.9.2-1
module load HDF5
# exe="./ramses_dmo_nomkdir3d_nvec32"

# module load HDF5
# module load Stages/2024
# module load GCC/12.3.0
# module load OpenMPI/4.1.5

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

exe="/p/scratch/hestiaeor/david/ahf-v1.0-115/bin/AHF-v1.0-115"

printf -v out "%03g" $SLURM_ARRAY_TASK_ID

# -K kills the job if a process returns a non-zero exit code
srun $exe /p/scratch/hestiaeor/david/AHF/$out/ahf.input
#=====END====
